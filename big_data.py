# -*- coding: utf-8 -*-
"""Big_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DWZU-s7CrBkN8qNDiACDCOBBzDDyoiqH
"""

# !pip install uszipcode
# !pip install python-Levenshtein

# virtual branch
# actual vs Address Branch
# uszipcode library is not very up to date...?
# Make final DF for predicting (maybe age should be in this if we can find data for it)
# Find zip codes for the desired counties  https://www.freemaptools.com/find-zip-codes-inside-state-county-city.htm
# age ?? https://data.census.gov/table/ACSDP1Y2022.DP05?q=average%20age&g=040XX00US12_050XX00US12095
# Make models
# Make functions/classes to make code better looking
# Turn zips into file?

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from uszipcode import SearchEngine
from dataclasses import dataclass
from sklearn.preprocessing import MinMaxScaler

from google.colab import drive
drive.mount('/content/drive')

@dataclass
class Params:
  '''
  All parameters used throughout the code
  '''
  sigma = 0.2 # determines how significant the weight a variable is
  scale_range = (0, 20)
  mismatch_weight = 10
  virtual_weight = 10
  test_size = 0.2

params = Params()

path_to_folder = '/content/drive/My Drive/Big Data Folder/'

branch_df = pd.read_excel(f'{path_to_folder}Branch_Level_Dataset.xlsx')
member_df = pd.read_csv(f'{path_to_folder}Member_Level_Dataset.csv')

"""## Adding Branch County to datasets"""

county_map = {
    'Addition Financial Arena': 'Orange',
    'Apopka': 'Orange',
    'Boone High School': 'Orange',
    'Colonial High School': 'Orange',
    'Downtown Campus': 'Orange',
    'East Orlando': 'Orange',
    'Edgewater High School': 'Orange',
    'Lake Nona': 'Orange',
    'MetroWest': 'Orange',
    'Mills': 'Orange',
    'Oak Ridge High School': 'Orange',
    'Ocoee High School': 'Orange',
    'Pine Hills': 'Orange',
    'South Orlando': 'Orange',
    'Timber Creek High School': 'Orange',
    'UCF Campus': 'Orange',
    'UCF Commons': 'Orange',
    'Winter Garden': 'Orange',
    'Altamonte Springs': 'Seminole',
    'Fern Park': 'Seminole',
    'Lake Brantley High School': 'Seminole',
    'Lake Howell High School': 'Seminole',
    'Lake Mary': 'Seminole',
    'Longwood': 'Seminole',
    'Oviedo': 'Seminole',
    'Sanford': 'Seminole',
    'Seminole State': 'Seminole',
    'Clermont': 'Lake',
    'Eustis': 'Lake',
    'Leesburg': 'Lake',
    'Kissimmee': 'Osceola',
    'Poinciana High School': 'Osceola',
    'St. Cloud': 'Osceola',
    'St. Cloud High School': 'Osceola',
    'The Loop': 'Osceola',
    'Merritt Island': 'Brevard',
    'Orange City': 'Volusia',
    'Poinciana': 'Polk',
    'Virtual Branch': None  # Virtual Branch does not have a county
}

branch_df['County'] = branch_df['BranchCategory'].map(county_map)
member_df['Branch_County'] = member_df['BranchCategory'].map(county_map)

branch_df.head()

# Grouping the branch data based on BranchCategory
grouped_branch = branch_df.groupby('BranchCategory').agg({
    'County': 'last',
    'ATM': 'sum',
    'Bill Payment': 'sum',
    'Cash': 'sum',
    'Draft': 'sum',
    'ACH': 'sum',
    'Fee': 'sum',
    'Credit/Debit Card': 'sum',
    'Home Banking': 'sum',
    'Dividend': 'sum',
    'EOM_TRANS_DATE': 'count',
}).rename(columns={'EOM_TRANS_DATE': 'Total_Transactions'}).reset_index()

# Note that some columns have very different range of variables, so scaling could be wise
grouped_branch.describe()

"""## Scaling Branch Data"""

# Scaling the columns to be within the same range before weighting them
scaler = MinMaxScaler(feature_range=params.scale_range)

columns_to_normalize = ['ATM', 'Bill Payment', 'Cash', 'Draft', 'ACH', 'Fee', 'Credit/Debit Card', 'Home Banking', 'Dividend']

scaled_grouped_branch = grouped_branch.copy()
scaled_grouped_branch[columns_to_normalize] = scaler.fit_transform(grouped_branch[columns_to_normalize])

scaled_grouped_branch.describe()

"""## Calculating Score for Each Branch"""

branch_weights = {
    'ATM': 1, # Standard transactions
    'Bill Payment': 1 + 1.5*params.sigma, # Slightly more important due to its link to loans
    'Cash': 1, # Standard transactions
    'Draft': 1, # Standard transactions
    'ACH': 1 + 2*params.sigma, # Primary banking indicator, more important
    'Fee': -1 - 1*params.sigma, # Negative impact
    'Credit/Debit Card': 1 + 1*params.sigma, # Standard transactions
    'Home Banking': 1 + 1*params.sigma, # Indicates engagement but not directly profitability
    'Dividend': 1 + 3*params.sigma, # High importance for profitability
    'Total_Transactions': 1, # Overall activity indicator
}

weighted_branch = scaled_grouped_branch.copy()
columns_for_scoring = columns_to_normalize + ['Total_Transactions']

# Apply weights
for column, weight in branch_weights.items():
    weighted_branch[column] *= weight

# Calculate the weighted score
weighted_branch['Branch_Score'] = weighted_branch[columns_for_scoring].sum(axis=1)

weighted_branch[['BranchCategory', 'Branch_Score']].sort_values(by='Branch_Score', ascending=False).reset_index()

"""## Modifying and Expanding Member Data Using Zip Code Data"""

unique_zips = member_df['address_zip'].unique()

search = SearchEngine()

zip_to_county_map = {}
zip_to_pop_map = {}
zip_to_popdens_map = {}
zip_to_rad_map = {}
zip_to_hunits_map = {}
zip_to_homeval_map = {}
zip_to_inc_map = {}
good_zips = []
bad_zips = []

for zip_code in unique_zips:
    zip_info = search.by_zipcode(zip_code)

    if zip_info:
        good_zips.append(zip_code)
        zip_to_county_map[zip_code] = zip_info.county
        zip_to_pop_map[zip_code] = zip_info.population
        zip_to_popdens_map[zip_code] = zip_info.population_density
        zip_to_rad_map[zip_code] = zip_info.radius_in_miles
        zip_to_hunits_map[zip_code] = zip_info.housing_units
        zip_to_homeval_map[zip_code] = zip_info.median_home_value
        zip_to_inc_map[zip_code] = zip_info.median_household_income

    else:
        bad_zips.append(zip_code)
        zip_to_county_map[zip_code] = np.nan
        zip_to_pop_map[zip_code] = np.nan
        zip_to_popdens_map[zip_code] = np.nan
        zip_to_rad_map[zip_code] = np.nan
        zip_to_hunits_map[zip_code] = np.nan
        zip_to_homeval_map[zip_code] = np.nan
        zip_to_inc_map[zip_code] = np.nan

# Adding member's home address county based on zip code
member_df['address_county'] = member_df['address_zip'].map(zip_to_county_map)
member_df['address_county'] = member_df['address_county'].str.replace(' County', '', regex=False)

# members we couldn't find data on their zip code (could be overseas, etc.)
missing_county_members = member_df[pd.isna(member_df['address_county'])].copy()

filled_members = member_df[pd.notna(member_df['address_county'])].copy()

filled_members['EOM_TRANS_DATE'] = pd.to_datetime(filled_members['EOM_TRANS_DATE'])

"""## Grouping member data so that each point is a unique member"""

grouped_members = filled_members.groupby('Unique_Member_Identifier').agg({
    # Note that we dropped n_accts since we are already adding up different accout types
    'age': 'last',
    'BranchCategory': 'last',
    'address_zip': 'last',
    'Branch_County': 'last',
    'address_county': 'last',
    'n_checking_accts': 'last',
    'n_savings_accts': 'last',
    'n_open_loans': 'last',
    'n_open_cds': 'last',
    'n_open_club_accts': 'last',
    'n_open_credit_cards': 'last',
    'ATMCount': 'sum',
    'BillPaymentCount': 'sum',
    'CashCount': 'sum',
    'DraftCount': 'sum',
    'ACHCount': 'sum',
    'FeeCount': 'sum',
    'Credit_DebitCount': 'sum',
    'Home_Banking': 'sum',
    'WireCount': 'sum',
    'DividendCount': 'sum',
    'EOM_TRANS_DATE': 'count',
}).rename(columns={'EOM_TRANS_DATE': 'Total_Transactions'}).reset_index()

# Adding Branch Scores
grouped_members = grouped_members.merge(weighted_branch[['BranchCategory', 'Branch_Score']],
                                        on='BranchCategory',
                                        how='left')

# When address county and branch county for a member differs
# (could indicate a branch near their zip code could be a good idea)
grouped_members['County_Mismatch'] = np.where(grouped_members['Branch_County'] != grouped_members['address_county'], 1, 0)
grouped_members['Virtual_Branch'] = np.where(grouped_members['BranchCategory'] == 'Virtual Branch', 1, 0)

grouped_members.head()

"""## Scaling Member Data"""

grouped_members.describe()

# Scaling the columns to be within the same range before weighting them
columns_to_normalize_members = ['n_checking_accts', 'n_savings_accts', 'n_open_loans', 'n_open_cds',
                                'n_open_club_accts', 'n_open_credit_cards', 'ATMCount', 'BillPaymentCount',
                                'CashCount', 'DraftCount', 'ACHCount', 'FeeCount', 'Credit_DebitCount',
                                'Home_Banking', 'WireCount', 'DividendCount', 'Branch_Score']

scaled_grouped_members = grouped_members.copy()
scaled_grouped_members[columns_to_normalize_members] = scaler.fit_transform(grouped_members[columns_to_normalize_members])

scaled_grouped_members.describe()

"""## Calculating a Score for Each Member"""

# For calculating Target Score:


# POSITIVES (adds to score):
# If home county differs from branch county, it could possibly mean that the home zip code needs a closer location.
# If Branch Score is high
# If Branch is Virtual (Maybe a near branch could work for them better)

# NEGATIVES (decreases score):
# Fee

member_weights = {
    'n_checking_accts': 1 + 1*params.sigma, # Standard account type
    'n_savings_accts': 1 + 2*params.sigma, # Indicator of stored funds
    'n_open_loans': 1 + 1.5*params.sigma, # Important for indebted customer base
    'n_open_cds': 1 + 3*params.sigma, # High-value accounts, significant for profitability
    'n_open_club_accts': 1, # Standard account type
    'n_open_credit_cards': 1 + 1*params.sigma, # Indicative of spending but not direct profitability
    'ATMCount': 1, # Standard transaction type
    'BillPaymentCount': 1 + 2*params.sigma, # Linked to loans, hence more important
    'CashCount': 1, # Standard transaction type
    'DraftCount': 1, # Standard transaction type
    'ACHCount': 1 + 3*params.sigma, # Primary banking indicator, more important
    'FeeCount': -1 - 1*params.sigma, # Negative impact
    'Credit_DebitCount': 1 + 1*params.sigma, # Standard transaction type
    'Home_Banking': 1 + 1*params.sigma, # Indicates engagement but not directly profitability
    'WireCount': 1 + 1.5*params.sigma, # Slightly more important due to larger transactions
    'DividendCount': 1 + 3*params.sigma, # High importance for profitability
    'Total_Transactions': 1, # Overall activity indicator
    'Branch_Score': 1 + 4*params.sigma, # Overall branch performance, highly important
    'County_Mismatch': params.mismatch_weight, # Binary Variable, so weight is a bit tricky
    'Virtual_Branch': params.virtual_weight, # Binary Variable, so weight is a bit tricky
}

weighted_members = scaled_grouped_members.copy()
columns_for_scoring_members = columns_to_normalize_members + ['Total_Transactions', 'County_Mismatch', 'Virtual_Branch']

# Apply weights
for column, weight in member_weights.items():
    weighted_members[column] *= weight

# Calculate the weighted score
weighted_members['Target_Score'] = weighted_members[columns_for_scoring_members].sum(axis=1)

weighted_members[['Unique_Member_Identifier', 'age', 'address_zip', 'Branch_County', 'Target_Score']].sort_values(by='Target_Score', ascending=False)

final_df = weighted_members[['age', 'address_zip', 'BranchCategory', 'Branch_County', 'address_county', 'Target_Score']].copy()

final_df['Population'] = final_df['address_zip'].map(zip_to_pop_map)
final_df['PopulationDensity'] = final_df['address_zip'].map(zip_to_popdens_map)
final_df['RadiusInMiles'] = final_df['address_zip'].map(zip_to_rad_map)
final_df['HousingUnits'] = final_df['address_zip'].map(zip_to_hunits_map)
final_df['MedianHomeValue'] = final_df['address_zip'].map(zip_to_homeval_map)
final_df['MedianHouseholdIncome'] = final_df['address_zip'].map(zip_to_inc_map)

final_df.head()

final_df.isna().sum()

# Some zip codes don't have the required data, so we will get rid of them since there aren't too many of them
# We still want data from virtual branch which has na values in Branch_County, so
# we won't drop those
final_df.dropna(subset=['Population'], inplace=True)

final_df.isna().sum()

len(final_df)

# Unique Zip Codes dataset, probably won't be used
zip_df = pd.DataFrame(good_zips, columns=['ZipCode'])

zip_df['County'] = zip_df['ZipCode'].map(zip_to_county_map)
zip_df['Population'] = zip_df['ZipCode'].map(zip_to_pop_map)
zip_df['PopulationDensity'] = zip_df['ZipCode'].map(zip_to_popdens_map)
zip_df['RadiusInMiles'] = zip_df['ZipCode'].map(zip_to_rad_map)
zip_df['HousingUnits'] = zip_df['ZipCode'].map(zip_to_hunits_map)
zip_df['MedianHomeValue'] = zip_df['ZipCode'].map(zip_to_homeval_map)
zip_df['MedianHouseholdIncome'] = zip_df['ZipCode'].map(zip_to_inc_map)

zip_df['County'] = zip_df['County'].str.replace(' County', '', regex=False)

zip_df.head()

"""## Making the dataset we want to predict score for at the end

### All the zip codes in the possible new branch locations
"""

orange = [32703,32704,32709,32710,32712,32751,32768,32777,32789,32790,32792,32793,32794,32798,32801,32802,32803,32804,32805,32806,32807,32808,32809,32810,32811,32812,32814,32816,32817,32818,32819,32820,32821,32822,32824,32825,32826,32827,32828,32829,32830,32831,32832,32833,32834,32835,32836,32837,32839,32853,32854,32855,32856,32857,32858,32859,32860,32861,32862,32867,32868,32869,32872,32877,32878,32885,32886,32887,32891,32896,32897,34734,34740,34760,34761,34777,34778,34786,34787]
osceola = [33848,34739,34741,34742,34743,34744,34745,34746,34747,34758,34769,34770,34771,34772,34773]
seminole = [32701,32707,32708,32714,32715,32716,32718,32719,32730,32732,32733,32745,32746,32747,32750,32752,32762,32765,32766,32771,32772,32773,32779,32791,32795,32799]
lake = [32102,32158,32159,32702,32726,32727,32735,32736,32756,32757,32767,32776,32778,32784,34705,34711,34712,34713,34714,34715,34729,34731,34736,34737,34748,34749,34753,34755,34756,34762,34788,34789,34797]
alachua = [32601,32602,32603,32604,32605,32606,32607,32608,32609,32610,32611,32612,32614,32615,32616,32618,32627,32631,32633,32635,32640,32641,32643,32653,32654,32655,32658,32662,32667,32669,32694]
brevard = [32754,32775,32780,32781,32783,32796,32815,32899,32901,32902,32903,32904,32905,32906,32907,32908,32909,32910,32911,32912,32919,32920,32922,32923,32924,32925,32926,32927,32931,32932,32934,32935,32936,32937,32940,32941,32949,32950,32951,32952,32953,32954,32955,32956,32959,32976]
duval = [32099,32201,32202,32203,32204,32205,32206,32207,32208,32209,32210,32211,32212,32214,32216,32217,32218,32219,32220,32221,32222,32223,32224,32225,32226,32227,32228,32229,32231,32232,32233,32234,32235,32236,32238,32239,32240,32241,32244,32245,32246,32247,32250,32254,32255,32256,32257,32258,32266,32277]
flagler = [32110,32135,32136,32137,32142,32143,32164]
hernando = [34601,34602,34603,34604,34605,34606,34607,34608,34609,34611,34613,34614,34636,34661]
highlands = [33825,33826,33852,33857,33862,33870,33871,33872,33875,33876,33960]
hillsborough = [33503,33508,33509,33510,33511,33527,33530,33534,33547,33548,33549,33550,33556,33558,33559,33563,33564,33565,33566,33567,33568,33569,33570,33571,33572,33573,33575,33578,33579,33583,33584,33586,33587,33592,33594,33595,33596,33598,33601,33602,33603,33604,33605,33606,33607,33608,33609,33610,33611,33612,33613,33614,33615,33616,33617,33618,33619,33620,33621,33622,33623,33624,33625,33626,33629,33630,33631,33633,33634,33635,33637,33646,33647,33650,33655,33660,33664,33672,33673,33674,33675,33677,33679,33680,33681,33682,33684,33685,33686,33687,33688,33689,33694]
indian = [32948,32957,32958,32960,32961,32962,32963,32964,32965,32966,32967,32968,32969,32970,32971,32978]
manatee = [34201,34202,34203,34204,34205,34206,34207,34208,34209,34210,34211,34212,34215,34216,34217,34218,34219,34220,34221,34222,34228,34243,34250,34251,34260,34264,34270,34280,34281,34282]
marion = [32111,32113,32133,32134,32179,32182,32183,32192,32195,32617,32634,32663,32664,32681,32686,34420,34421,34430,34431,34432,34470,34471,34472,34473,34474,34475,34476,34477,34478,34479,34480,34481,34482,34483,34488,34489,34491,34492]
martin = [33455,33475,34956,34957,34958,34990,34991,34992,34994,34995,34996,34997]
okeechobee = [34972,34973,34974]
pasco = [33523,33524,33525,33526,33537,33539,33540,33541,33542,33543,33544,33545,33574,33576,33593,34610,34637,34638,34639,34652,34653,34654,34655,34656,34667,34668,34669,34673,34674,34679,34680,34690,34691,34692]
pinellas = [33701,33702,33703,33704,33705,33706,33707,33708,33709,33710,33711,33712,33713,33714,33715,33716,33729,33730,33731,33732,33733,33734,33736,33738,33740,33741,33742,33743,33744,33747,33755,33756,33757,33758,33759,33760,33761,33762,33763,33764,33765,33766,33767,33769,33770,33771,33772,33773,33774,33775,33776,33777,33778,33779,33780,33781,33782,33784,33785,33786,34660,34677,34681,34682,34683,34684,34685,34688,34689,34695,34697,34698]
polk = [33801,33802,33803,33804,33805,33806,33807,33809,33810,33811,33812,33813,33815,33820,33823,33827,33830,33831,33835,33836,33837,33838,33839,33840,33841,33843,33844,33845,33846,33847,33849,33850,33851,33853,33854,33855,33856,33858,33859,33860,33863,33867,33868,33877,33880,33881,33882,33883,33884,33885,33888,33896,33897,33898,34759]
sarasota = [33801,33802,33803,33804,33805,33806,33807,33809,33810,33811,33812,33813,33815,33820,33823,33827,33830,33831,33835,33836,33837,33838,33839,33840,33841,33843,33844,33845,33846,33847,33849,33850,33851,33853,33854,33855,33856,33858,33859,33860,33863,33867,33868,33877,33880,33881,33882,33883,33884,33885,33888,33896,33897,33898,34759]
stJohns = [32004,32033,32080,32081,32082,32084,32085,32086,32092,32095,32145,32259,32260]
stLucie = [34945,34946,34947,34948,34949,34950,34951,34952,34953,34954,34979,34981,34982,34983,34984,34985,34986,34987,34988]
sumter = [32162,32163,33513,33514,33521,33538,33585,33597,34484,34785]
volusia = [32105,32114,32115,32116,32117,32118,32119,32120,32121,32122,32123,32124,32125,32126,32127,32128,32129,32130,32132,32141,32168,32169,32170,32173,32174,32175,32176,32180,32190,32198,32706,32713,32720,32721,32722,32723,32724,32725,32728,32738,32739,32744,32753,32759,32763,32764,32774]

all_pred_zips = orange + osceola + seminole + lake + alachua + brevard + duval + flagler + hernando + highlands + hillsborough + indian + manatee + marion + martin + okeechobee + pasco + pinellas + polk + sarasota + stJohns + stLucie + sumter + volusia



zip_to_pop_map_pred = {}
zip_to_popdens_map_pred = {}
zip_to_rad_map_pred = {}
zip_to_hunits_map_pred = {}
zip_to_homeval_map_pred = {}
zip_to_inc_map_pred = {}
good_zips_pred = []
bad_zips_pred = []

for zip_code in all_pred_zips:
    zip_info = search.by_zipcode(zip_code)

    if zip_info:
        good_zips_pred.append(zip_code)
        zip_to_pop_map_pred[zip_code] = zip_info.population
        zip_to_popdens_map_pred[zip_code] = zip_info.population_density
        zip_to_rad_map_pred[zip_code] = zip_info.radius_in_miles
        zip_to_hunits_map_pred[zip_code] = zip_info.housing_units
        zip_to_homeval_map_pred[zip_code] = zip_info.median_home_value
        zip_to_inc_map_pred[zip_code] = zip_info.median_household_income

    else:
        bad_zips_pred.append(zip_code)
        zip_to_pop_map_pred[zip_code] = np.nan
        zip_to_popdens_map_pred[zip_code] = np.nan
        zip_to_rad_map_pred[zip_code] = np.nan
        zip_to_hunits_map_pred[zip_code] = np.nan
        zip_to_homeval_map_pred[zip_code] = np.nan
        zip_to_inc_map_pred[zip_code] = np.nan

len(bad_zips_pred)

# Unique Zip Codes dataset, probably won't be used
pred_df = pd.DataFrame(good_zips_pred, columns=['address_zip'])

pred_df['Population'] = pred_df['address_zip'].map(zip_to_pop_map_pred)
pred_df['PopulationDensity'] = pred_df['address_zip'].map(zip_to_popdens_map_pred)
pred_df['RadiusInMiles'] = pred_df['address_zip'].map(zip_to_rad_map_pred)
pred_df['HousingUnits'] = pred_df['address_zip'].map(zip_to_hunits_map_pred)
pred_df['MedianHomeValue'] = pred_df['address_zip'].map(zip_to_homeval_map_pred)
pred_df['MedianHouseholdIncome'] = pred_df['address_zip'].map(zip_to_inc_map_pred)

pred_df.head()

pred_df.isna().sum()

# We do not have enough data on 311 zip codes, so we have to drop them
pred_df.dropna(inplace=True)
len(pred_df)

